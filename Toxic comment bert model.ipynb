{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed968aa",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cb1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4db4c",
   "metadata": {},
   "source": [
    "### Setting up Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af32f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 320\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb2d8e",
   "metadata": {},
   "source": [
    "## Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bfa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cleaned_data.csv')\n",
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_data.dropna(inplace=True)\n",
    "#train_data['labels'] = train_data[label_columns].values.tolist()\n",
    "#train_data = train_data.drop(label_columns+['id','comment_text','cleaned_text_ps','cleaned_text_sb','comment_length'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1daf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.to_csv('train_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2edf32a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig Dataset: (159525, 12)\n",
      "Training Dataset: (135596, 12)\n",
      "Validation Dataset: (23929, 12)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.85\n",
    "\n",
    "train_df = train_data.sample(frac=train_size, random_state=42)\n",
    "val_df = train_data.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Orig Dataset: {}\".format(train_data.shape))\n",
    "print(\"Training Dataset: {}\".format(train_df.shape))\n",
    "print(\"Validation Dataset: {}\".format(val_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214c1f0",
   "metadata": {},
   "source": [
    "###  Creating custom pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e0a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "class ToxicCommentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        labels = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb4c04",
   "metadata": {},
   "source": [
    "### Creating dataloaders from the custom datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a17cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 4\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 4\n",
    "                }\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_data = ToxicCommentDataset(tokenizer, train_df['comment_text'], train_df[label_columns].values, MAX_LEN)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_data, [int(len(train_data) * 0.8), len(train_data) - int(len(train_data) * 0.8)])\n",
    "train_dataloader = DataLoader(train_dataset, **train_params)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_data = ToxicCommentDataset(tokenizer, val_df['comment_text'], val_df[label_columns].values, MAX_LEN)\n",
    "val_dataloader = DataLoader(val_data, **val_params)\n",
    "\n",
    "# Split data into train and test sets\n",
    "test_dataloader = DataLoader(test_dataset, **val_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f9841c",
   "metadata": {},
   "source": [
    "###  Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91859f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Define BERT model architecture\\nclass BERTModel(nn.Module):\\n    def __init__(self):\\n        super(BERTModel, self).__init__()\\n        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\\n        self.classifier = torch.nn.Sequential(\\n            torch.nn.Linear(768, 768),\\n            torch.nn.ReLU(),\\n            torch.nn.Dropout(0.1),\\n            torch.nn.Linear(768, 6)\\n        )\\n        \\n    def forward(self, input_ids, attention_mask, token_type_ids):\\n        output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\\n        hidden_state = output_1[0]\\n        out = hidden_state[:, 0]\\n        out = self.classifier(out)\\n        return out\\n    \\nmodel = BERTModel()\\nmodel.to(DEVICE);\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Define BERT model architecture\n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(768, 6)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        out = hidden_state[:, 0]\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "model = BERTModel()\n",
    "model.to(DEVICE);\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7581e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Smaller version of the Bert Transformer model.\n",
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        \n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(768, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        out = hidden_state[:, 0]\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "model = DistilBERTClass()\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e3d9e",
   "metadata": {},
   "source": [
    "### Training, predicting and evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f563f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(epochs=EPOCHS, lr=LEARNING_RATE, device=DEVICE):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    batch_size = 32\n",
    "    accumulation_steps = 4\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "   \n",
    "            # Compute the loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss = loss / accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                model.zero_grad()\n",
    "                \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Batch {i + 1} / {len(train_dataloader)}, Loss: {total_loss / (i + 1)}')\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {total_loss / len(train_dataloader)}')\n",
    "        \n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            logits = outputs.detach().cpu().numpy()\n",
    "            label_ids = labels.to(device).numpy()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(logits)\n",
    "            true_labels.extend(label_ids)\n",
    "\n",
    "    # Compute metrics\n",
    "    pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    avg_loss = loss.item() / len(dataloader)\n",
    "\n",
    "    return avg_loss, true_labels, pred_labels\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            predictions.append(predicted.cpu().detach().numpy())\n",
    "\n",
    "    predictions = torch.from_numpy(np.concatenate(predictions, axis=0))\n",
    "    return predictions\n",
    "\n",
    "def predict_text(text, model, tokenizer, device=DEVICE):\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    # Tokenize the text and convert to input IDs\n",
    "    input_ids = torch.tensor(encoded_text['input_ids'], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(encoded_text['attention_mask'], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.tensor(encoded_text['token_type_ids'], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate the attention mask\n",
    "    attention_mask = (input_ids != 0).float()\n",
    "\n",
    "    # Make the prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "    # Convert the logits to probabilities\n",
    "    probs = torch.sigmoid(outputs)\n",
    "\n",
    "    # Convert the probabilities to binary predictions\n",
    "    preds = probs.detach().cpu().numpy()\n",
    "\n",
    "    # Convert the binary predictions to class labels\n",
    "    labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    results = {label: round(pred.item(),4) for label, pred in zip(labels, preds[0])}\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "878740fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Training and saving model state\\ntrain(EPOCHS)\\nPATH = \"toxic_comment_1.pkl\"\\ntorch.save(model.state_dict(), PATH)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training and saving model state\n",
    "\"\"\"\n",
    "train(EPOCHS)\n",
    "PATH = \"toxic_comment_1.pkl\"\n",
    "torch.save(model.state_dict(), PATH)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc88fb4",
   "metadata": {},
   "source": [
    "### Loading Saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0b92fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBERTClass()\n",
    "model.load_state_dict(torch.load(\"toxic_comment_1.pkl\", map_location=torch.device('cpu'))) #ignor the map_location paramerer if on a gpu\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72457b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': 0.6366,\n",
       " 'severe_toxic': 0.0076,\n",
       " 'obscene': 0.1221,\n",
       " 'threat': 0.0095,\n",
       " 'insult': 0.1998,\n",
       " 'identity_hate': 0.0225}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"nobody it sucks\"\n",
    "predict_text(text ,model, tokenizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fd931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
